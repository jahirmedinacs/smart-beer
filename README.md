# Proyecto: CervecerÃ­a AutÃ³noma - Sistema de Monitoreo y Control

[Este proyecto](https://github.com/jahirmedinacs/smart-beer) implementa un sistema para monitorear, controlar y analizar el proceso de elaboraciÃ³n de cerveza artesanal. La arquitectura estÃ¡ diseÃ±ada para ser escalable, robusta y de alta disponibilidad, permitiendo el anÃ¡lisis de datos en tiempo real e histÃ³rico.

![](./assets/banner.png)

<br>

--- 

## Arquitectura del Sistema
El sistema se distribuye en mÃºltiples Nodos Sensores/Actuadores por cada olla de cocciÃ³n y un par de Servidores Centrales redundantes. Esta configuraciÃ³n garantiza que no exista un Ãºnico punto de fallo.

* Nodos Sensores (3 por Olla): Cada nodo (Raspberry Pi) captura un conjunto de datos y puede ejecutar acciones. EnvÃ­an sus reportes a un punto de entrada centralizado.

* Servidores Centrales (2 para Redundancia): Dos Mini PCs idÃ©nticas operan en paralelo. Un balanceador de carga distribuye el trÃ¡fico de ingesta de datos entre ellas. Las bases de datos se mantienen sincronizadas constantemente mediante replicaciÃ³n, asegurando la integridad y disponibilidad de la informaciÃ³n.

![](./assets/diagram.svg)

<br>

--- 

## Arquitectura de Datos y Estrategia de Cacheo
El flujo de datos estÃ¡ diseÃ±ado en capas para optimizar la velocidad de respuesta y la eficiencia del almacenamiento.

1. Ingesta: El Servicio de Ingesta recibe un reporte .json.

2. Capa Caliente (Cache - Redis): Inmediatamente, el servicio escribe los datos en Redis. Esta base de datos en memoria actÃºa como un cachÃ© ultrarrÃ¡pido que almacena los datos de los Ãºltimos 3 dÃ­as. El dashboard de tiempo real consume los datos directamente desde aquÃ­, garantizando una respuesta casi instantÃ¡nea.

3. Capa Tibia (Persistente - MongoDB): SimultÃ¡neamente, los mismos datos se escriben en MongoDB. Esta base de datos sirve como el repositorio principal para datos histÃ³ricos a mediano plazo. Las consultas a la vista de "HistÃ³rico" se realizan contra esta base de datos.

4. Capa FrÃ­a (Archivo a Largo Plazo - Cassandra): Un proceso automatizado (no implementado en el prototipo inicial) se encargarÃ­a de mover los datos con mÃ¡s de 30 dÃ­as de antigÃ¼edad desde MongoDB hacia Cassandra. Esta capa estÃ¡ optimizada para almacenar volÃºmenes masivos de datos y realizar anÃ¡lisis complejos a gran escala.

Esta estrategia asegura que las consultas mÃ¡s frecuentes (datos recientes) sean extremadamente rÃ¡pidas, sin sacrificar la capacidad de almacenar y analizar grandes volÃºmenes de datos histÃ³ricos de forma eficiente.

## ğŸ“œ Estructura de Directorios
La estructura base del proyecto se mantiene, pero la configuraciÃ³n y despliegue deben considerar la nueva arquitectura.

```bash
â”œâ”€â”€ autonomous_brewing
â”‚Â Â  â”œâ”€â”€ central_server
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ api_backend
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ brewing_project
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ settings.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ urls.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_api
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ management
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ commands
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ urls.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ views.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ manage.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metrics
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ requirements.txt
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ database
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ docker-compose.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ frontend
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ history.html
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ static
â”‚Â Â  â”‚Â Â  â”‚Â Â      â”œâ”€â”€ css
â”‚Â Â  â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ styles.css
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ js
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ app.js
â”‚Â Â  â”‚Â Â  â”‚Â Â          â””â”€â”€ history.js
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ incoming_reports
â”‚Â Â  â”‚Â Â  â””â”€â”€ services
â”‚Â Â  â”‚Â Â      â””â”€â”€ data_ingestion
â”‚Â Â  â”‚Â Â          â”œâ”€â”€ ingest.py
â”‚Â Â  â”‚Â Â          â””â”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ raspberry_pi
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ reports
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ sensor_simulator.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ sync_reports.sh
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ README.md
â””â”€â”€ setup.sh

19 directories, 19 files
```


1. raspberry_pi (Nodo Sensor)

* reports/: AlmacÃ©n temporal de reportes JSON.

* sensor_simulator.py: Script de simulaciÃ³n. En un entorno real, este script leerÃ­a los sensores fÃ­sicos.

* sync_reports.sh: Script rsync. Importante: Ahora debe apuntar a la direcciÃ³n IP virtual (VIP) del balanceador de carga, no a un servidor individual.

2. central_server (Servidor Central)
La configuraciÃ³n de este directorio debe ser idÃ©ntica en ambos servidores para garantizar la consistencia.

* incoming_reports/: Carpeta de aterrizaje para los reportes.

* database/:

  * docker-compose.yml: Define los contenedores de las bases de datos. La configuraciÃ³n debe incluir los parÃ¡metros para la replicaciÃ³n entre el servidor 1 y el servidor 2 (por ejemplo, un replica set en MongoDB).

* services/data_ingestion/:

  * ingest.py: El microservicio de ingesta se ejecuta en ambos servidores, procesando los archivos que recibe del balanceador.

* api_backend/:

  * El backend de Django se ejecuta en ambos servidores, consultando su instancia local de la base de datos (que estÃ¡ replicada).

* frontend/:

  * Los archivos del frontend son servidos por ambos servidores o desde un servicio de hosting estÃ¡tico. Las peticiones a la API deben dirigirse al balanceador de carga.

![](./assets/image.png)

<br>

--- 

## ğŸ› ï¸ Herramientas y TecnologÃ­as

* Lenguajes: Python, Bash, HTML, CSS, JavaScript.

* Framework Backend: Django & Django REST Framework.

* Bases de Datos:

  * Redis: Cache en memoria (configurado en modo clÃºster o con Sentinel para alta disponibilidad).

  * MongoDB: Base de datos NoSQL (configurada como un Replica Set).

  * Cassandra: Base de datos distribuida (naturalmente redundante).

* Transferencia de Datos: rsync sobre SSH.

* ContenerizaciÃ³n: Docker & Docker Compose.

* Redundancia y Balanceo: Herramientas como HAProxy, Nginx (como balanceador de carga) o soluciones a nivel de DNS.

* LibrerÃ­as Python Clave: redis-py, pymongo, watchdog, django.

## ğŸš€ Puesta en Marcha
La ejecuciÃ³n del sistema ahora requiere pasos adicionales para configurar la alta disponibilidad:

1. Configurar el Balanceador de Carga: Desplegar un balanceador (ej. HAProxy) que distribuya el trÃ¡fico a los dos servidores centrales.

2. Configurar la ReplicaciÃ³n de BD: Modificar el docker-compose.yml para establecer la replicaciÃ³n entre las instancias de MongoDB y Redis en ambos servidores.

3. Desplegar Servidores Centrales: Ejecutar la pila de software (Docker, servicio de ingesta, API) en ambos servidores.

4. Configurar Nodos Sensores: Modificar el script sync_reports.sh en cada Raspberry Pi para que apunte a la IP del balanceador de carga.

5. Iniciar el Sistema: Ejecutar los simuladores en los nodos sensores y verificar que los datos fluyan a travÃ©s del balanceador y se repliquen correctamente en las bases de datos.

6. Acceder al Frontend: Abrir la interfaz web, que ahora harÃ¡ peticiones a la IP del balanceador.